[
    {
        "name": "inferkit_text_gen",
        "description": "InferKit's text generation tool takes text you provide and generates what it thinks comes next.",
        "parameters": {
            "type": "object",
            "properties": {
                "prompt": {
                    "type": "string",
                    "desciption": "Context for the generator to build off of. The response will return what the neural network thinks comes next."
                },
                "length": {
                    "type": "int",
                    "desciption": "Maximum number of characters (Unicode code points) to generate."
                },
                "topP": {
                    "type": "float",
                    "desciption": "A probability threshold for discarding unlikely text in the sampling process. For example, 0.9 means that at each step only the most likely tokens with probabilities adding up to 90% will be sampled from. "
                },
                "temperature": {
                    "type": "float",
                    "desciption": "Controls the randomness of samplingâ€”the \"creativity\".Values greater than 1 will increase the chance of sampling unusual text.Values between 0 and 1 will cause the network to prefer the text it thinks is most likely. "
                }
            }
        },
        "required": [
            "prompt",
            "length"
        ]
    },
    {
        "name": "cohere_text_gen",
        "description": "Generates realistic text conditioned on a given input.",
        "parameters": {
            "type": "object",
            "properties": {
                "prompt": {
                    "type": "string",
                    "desciption": "Context for the generator to build off of. The response will return what the neural network thinks comes next."
                },
                "max_tokens": {
                    "type": "int",
                    "desciption": "Maximum number of characters (Unicode code points) to generate."
                },
                "p": {
                    "type": "float",
                    "desciption": "Ensures that only the most likely tokens, with total probability mass of p, are considered for generation at each step. If both k and p are enabled, p acts after k."
                },
                "k": {
                    "type": "int",
                    "desciption": "Ensures only the top k most likely tokens are considered for generation at each step."
                },
                "truncate": {
                    "type": "str",
                    "desciption": "One of NONE|START|END to specify how the API will handle inputs longer than the maximum token length.Passing START will discard the start of the input. END will discard the end of the input. In both cases, input is discarded until the remaining input is exactly the maximum input token length for the model.If NONE is selected, when the input exceeds the maximum input token length an error will be returned."
                },
                "temperature": {
                    "type": "float",
                    "desciption": "A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations. See Temperature for more details."
                },
                "return_likelihoods": {
                    "type": "str",
                    "desciption": "One of GENERATION|ALL|NONE to specify how and if the token likelihoods are returned with the response. Defaults to NONE.If GENERATION is selected, the token likelihoods will only be provided for generated text. If ALL is selected, the token likelihoods will be provided both for the prompt and the generated text."
                },
                "presence_penalty": {
                    "type": "float",
                    "desciption": "Used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation."
                },
                "num_generations": {
                    "type": "int",
                    "desciption": "The maximum number of generations that will be returned. Defaults to 1, min value of 1, max value of 5."
                }
            }
        },
        "required": [
            "prompt"
        ]
    },
    {
        "name": "ask_to_user",
        "description": "You can ask user for guidance when you think you need more information to handle the task, but you should use this tool as less as you can.",
        "parameters": {
            "type": "object",
            "properties": {
                "question": {
                    "type": "string",
                    "desciption": "The question you want to ask to user."
                }
            }
        },
        "required": [
            "question"
        ]
    },
    {
        "name": "finish",
        "description": "Finish the task and give your answer.",
        "parameters": {
            "type": "object",
            "properties": {
                "answer": {
                    "type": "string",
                    "desciption": "Your answer for the task."
                }
            }
        },
        "required": [
            "answer"
        ]
    }
]